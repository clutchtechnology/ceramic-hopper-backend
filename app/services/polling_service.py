# ============================================================
# æ–‡ä»¶è¯´æ˜: polling_service.py - ä¼˜åŒ–ç‰ˆæ•°æ®è½®è¯¢æœåŠ¡
# ============================================================
# ä¼˜åŒ–ç‚¹:
#   1. PLC é•¿è¿æ¥ (é¿å…é¢‘ç¹è¿æ¥/æ–­å¼€)
#   2. æ‰¹é‡å†™å…¥ (30 æ¬¡è½®è¯¢ç¼“å­˜åæ‰¹é‡å†™å…¥)
#   3. æœ¬åœ°é™çº§ç¼“å­˜ (InfluxDB æ•…éšœæ—¶å†™å…¥ SQLite)
#   4. è‡ªåŠ¨é‡è¯•æœºåˆ¶ (ç¼“å­˜æ•°æ®è‡ªåŠ¨é‡è¯•)
#   5. å†…å­˜ç¼“å­˜ (ä¾› API ç›´æ¥è¯»å–æœ€æ–°æ•°æ®)
#   6. Mockæ¨¡å¼æ”¯æŒ (ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ›¿ä»£çœŸå®PLC)
# ============================================================

import asyncio
import yaml
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Any, List, Tuple
from collections import deque, defaultdict

from config import get_settings
from app.core.timezone_utils import now_beijing, beijing_isoformat
from app.core.influxdb import build_point, write_points_batch, check_influx_health
from app.core.local_cache import get_local_cache, CachedPoint
from app.plc.plc_manager import get_plc_manager
from app.plc.parser_hopper_4 import Hopper4Parser
from app.tools import get_converter, CONVERTER_MAP

settings = get_settings()

# Mockæ•°æ®ç”Ÿæˆå™¨ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œä»…åœ¨mockæ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
_mock_generator = None

# è½®è¯¢ä»»åŠ¡å¥æŸ„
_polling_task: Optional[asyncio.Task] = None
_retry_task: Optional[asyncio.Task] = None
_cleanup_task: Optional[asyncio.Task] = None  # ğŸ”§ æ·»åŠ æ¸…ç†ä»»åŠ¡å¥æŸ„
_is_running = False

# è§£æå™¨å®ä¾‹
_parsers: Dict[int, Any] = {}

# DBæ˜ å°„é…ç½®
_db_mappings: List[Dict[str, Any]] = []

# ============================================================
# æœ€æ–°æ•°æ®ç¼“å­˜ (ä¾› API ç›´æ¥è¯»å–ï¼Œé¿å…æŸ¥è¯¢æ•°æ®åº“)
# ============================================================
import threading
_data_lock = threading.Lock()  # ğŸ”§ æ·»åŠ æ•°æ®è®¿é—®é”
_latest_data: Dict[str, Any] = {}  # æœ€æ–°çš„è®¾å¤‡æ•°æ® {device_id: {...}}
_latest_timestamp: Optional[datetime] = None  # æœ€æ–°æ•°æ®æ—¶é—´æˆ³

# ============================================================
# æ‰¹é‡å†™å…¥ç¼“å­˜
# ============================================================
_point_buffer: deque = deque(maxlen=1000)  # æœ€å¤§ç¼“å­˜ 1000 ä¸ªç‚¹
_buffer_count = 0
_batch_size = 12  # ğŸ”§ [CRITICAL] 12æ¬¡è½®è¯¢åæ‰¹é‡å†™å…¥ï¼ˆçº¦60ç§’ä¸€æ¬¡ï¼‰
                   # ç¼©å°æ‰¹æ¬¡é¿å…æ‰¹é‡å†™å…¥é˜»å¡ API è¯·æ±‚è¿‡ä¹…
_poll_interval = settings.plc_poll_interval

# ğŸ”§ [NEW] åå°å†™å…¥ä»»åŠ¡æ§åˆ¶
_write_queue: asyncio.Queue = None  # å†™å…¥é˜Ÿåˆ—ï¼ˆå¼‚æ­¥ï¼‰
_write_task: Optional[asyncio.Task] = None  # åå°å†™å…¥ä»»åŠ¡
_write_in_progress = False  # æ˜¯å¦æ­£åœ¨å†™å…¥

# ============================================================
# ç»Ÿè®¡ä¿¡æ¯
# ============================================================
_stats = {
    "total_polls": 0,
    "successful_writes": 0,
    "failed_writes": 0,
    "cached_points": 0,
    "retry_success": 0,
    "last_write_time": None,
    "last_retry_time": None,
    "status_errors": 0,  # çŠ¶æ€é”™è¯¯è®¡æ•°
}


# ------------------------------------------------------------
# 1. _load_db_mappings() - åŠ è½½DBæ˜ å°„é…ç½®
# ------------------------------------------------------------
def _load_db_mappings() -> List[Tuple[int, int]]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½DBæ˜ å°„
    
    Returns:
        List[Tuple[int, int]]: [(db_number, total_size), ...]
    """
    global _db_mappings, _batch_size, _poll_interval
    
    config_path = Path("configs/db_mappings.yaml")
    
    if not config_path.exists():
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return [(6, 554)]
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        _db_mappings = config.get('db_mappings', [])

        # åŠ è½½è½®è¯¢é…ç½®
        polling_config = config.get('polling_config', {})
        poll_interval = polling_config.get('poll_interval', settings.plc_poll_interval)
        batch_write_size = polling_config.get('batch_write_size', settings.batch_write_size)

        _poll_interval = poll_interval
        _batch_size = batch_write_size

        print(f"ğŸ“Š è½®è¯¢é—´éš”: {poll_interval}ç§’")
        print(f"ğŸ“¦ æ‰¹é‡å†™å…¥é—´éš”: {batch_write_size}æ¬¡")
        
        # åªè¿”å›å¯ç”¨çš„DBå—é…ç½®
        enabled_configs = [
            (mapping['db_number'], mapping['total_size'])
            for mapping in _db_mappings
            if mapping.get('enabled', True)
        ]
        
        print(f"âœ… åŠ è½½DBæ˜ å°„é…ç½®: {len(enabled_configs)}ä¸ªDBå—")
        for db_num, size in enabled_configs:
            mapping = next(m for m in _db_mappings if m['db_number'] == db_num)
            print(f"   - DB{db_num}: {mapping['db_name']} ({size}å­—èŠ‚)")
        
        return enabled_configs
    
    except Exception as e:
        print(f"âŒ åŠ è½½DBæ˜ å°„é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return [(6, 554)]


# ------------------------------------------------------------
# 2. _init_parsers() - åˆå§‹åŒ–è§£æå™¨ï¼ˆåŠ¨æ€ï¼‰
# ------------------------------------------------------------
def _init_parsers():
    """æ ¹æ®é…ç½®æ–‡ä»¶åŠ¨æ€åˆå§‹åŒ–è§£æå™¨"""
    global _parsers, _db_mappings
    
    parser_classes = {
        'Hopper4Parser': Hopper4Parser
    }
    
    _parsers = {}
    
    for mapping in _db_mappings:
        if not mapping.get('enabled', True):
            continue
        
        db_number = mapping['db_number']
        parser_class_name = mapping.get('parser_class')
        
        if parser_class_name in parser_classes:
            _parsers[db_number] = parser_classes[parser_class_name]()
            print(f"   âœ… DB{db_number} -> {parser_class_name}")
        else:
            print(f"   âš ï¸  æœªçŸ¥çš„è§£æå™¨ç±»: {parser_class_name}")


# ============================================================
# æ‰¹é‡å†™å…¥ & æœ¬åœ°ç¼“å­˜
# ============================================================
def _flush_buffer():
    """åˆ·æ–°ç¼“å­˜ï¼šå°†æ•°æ®æ”¾å…¥å¼‚æ­¥å†™å…¥é˜Ÿåˆ—ï¼ˆä¸é˜»å¡ï¼‰"""
    global _buffer_count, _write_queue
    
    if len(_point_buffer) == 0:
        return
    
    # è½¬æ¢ä¸º Point åˆ—è¡¨
    points = list(_point_buffer)
    _point_buffer.clear()
    _buffer_count = 0
    
    # ğŸ”§ [CRITICAL] å°†æ•°æ®æ”¾å…¥å¼‚æ­¥é˜Ÿåˆ—ï¼Œä¸é˜»å¡å½“å‰çº¿ç¨‹
    if _write_queue is not None:
        try:
            _write_queue.put_nowait(points)
            print(f"ğŸ“¤ å·²å°† {len(points)} ä¸ªæ•°æ®ç‚¹åŠ å…¥å†™å…¥é˜Ÿåˆ—")
        except asyncio.QueueFull:
            print(f"âš ï¸ å†™å…¥é˜Ÿåˆ—å·²æ»¡ï¼Œæ•°æ®è½¬å­˜åˆ°æœ¬åœ°ç¼“å­˜")
            _save_to_local_cache(points)
    else:
        # é˜Ÿåˆ—æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŒæ­¥å†™å…¥ï¼ˆé™çº§ï¼‰
        _sync_write_to_influx(points)


def _sync_write_to_influx(points: List):
    """åŒæ­¥å†™å…¥ InfluxDBï¼ˆé™çº§æ¨¡å¼ï¼‰"""
    global _stats
    
    healthy, msg = check_influx_health()
    
    if healthy:
        success, err = write_points_batch(points)
        if success:
            _stats["successful_writes"] += len(points)
            _stats["last_write_time"] = beijing_isoformat()
            print(f"âœ… æ‰¹é‡å†™å…¥ {len(points)} ä¸ªæ•°æ®ç‚¹åˆ° InfluxDB")
        else:
            print(f"âŒ InfluxDB å†™å…¥å¤±è´¥: {err}ï¼Œè½¬å­˜åˆ°æœ¬åœ°ç¼“å­˜")
            _save_to_local_cache(points)
    else:
        print(f"âš ï¸ InfluxDB ä¸å¯ç”¨ ({msg})ï¼Œæ•°æ®å†™å…¥æœ¬åœ°ç¼“å­˜")
        _save_to_local_cache(points)


async def _background_writer():
    """ğŸ”§ [NEW] åå°å†™å…¥ä»»åŠ¡ - å¼‚æ­¥å¤„ç†å†™å…¥é˜Ÿåˆ—ï¼Œä¸é˜»å¡ API"""
    global _stats, _write_in_progress, _write_queue
    
    print("ğŸš€ åå°å†™å…¥ä»»åŠ¡å·²å¯åŠ¨")
    
    while _is_running:
        try:
            # ç­‰å¾…é˜Ÿåˆ—ä¸­çš„æ•°æ®ï¼ˆæœ€å¤šç­‰å¾… 1 ç§’ï¼Œå…è®¸æ£€æŸ¥ _is_runningï¼‰
            try:
                points = await asyncio.wait_for(_write_queue.get(), timeout=1.0)
            except asyncio.TimeoutError:
                continue
            
            if not points:
                continue
            
            _write_in_progress = True
            
            # æ£€æŸ¥ InfluxDB å¥åº·çŠ¶æ€
            healthy, msg = check_influx_health()
            
            if healthy:
                # å°è¯•å†™å…¥ InfluxDB
                success, err = write_points_batch(points)
                
                if success:
                    _stats["successful_writes"] += len(points)
                    _stats["last_write_time"] = beijing_isoformat()
                    print(f"âœ… [åå°] æ‰¹é‡å†™å…¥ {len(points)} ä¸ªæ•°æ®ç‚¹åˆ° InfluxDB")
                else:
                    print(f"âŒ [åå°] InfluxDB å†™å…¥å¤±è´¥: {err}ï¼Œè½¬å­˜åˆ°æœ¬åœ°ç¼“å­˜")
                    _save_to_local_cache(points)
            else:
                # InfluxDB ä¸å¯ç”¨ï¼Œä¿å­˜åˆ°æœ¬åœ°
                print(f"âš ï¸ [åå°] InfluxDB ä¸å¯ç”¨ ({msg})ï¼Œæ•°æ®å†™å…¥æœ¬åœ°ç¼“å­˜")
                _save_to_local_cache(points)
            
            _write_in_progress = False
            _write_queue.task_done()
            
        except asyncio.CancelledError:
            print("ğŸ›‘ åå°å†™å…¥ä»»åŠ¡å·²å–æ¶ˆ")
            break
        except Exception as e:
            print(f"âŒ [åå°] å†™å…¥ä»»åŠ¡å¼‚å¸¸: {e}")
            _write_in_progress = False
            await asyncio.sleep(1)  # å‡ºé”™åç­‰å¾… 1 ç§’å†ç»§ç»­
    
    print("ğŸ›‘ åå°å†™å…¥ä»»åŠ¡å·²åœæ­¢")


def _save_to_local_cache(points: List):
    """ä¿å­˜æ•°æ®ç‚¹åˆ°æœ¬åœ° SQLite ç¼“å­˜"""
    global _stats
    
    cache = get_local_cache()
    cached_points = []
    
    for point in points:
        # æå– Point å¯¹è±¡çš„ä¿¡æ¯
        cached_point = CachedPoint(
            measurement=point._name,
            tags={k: v for k, v in point._tags.items()},
            fields={k: v for k, v in point._fields.items()},
            timestamp=point._time.isoformat() if point._time else beijing_isoformat()
        )
        cached_points.append(cached_point)
    
    saved_count = cache.save_points(cached_points)
    _stats["cached_points"] += saved_count
    _stats["failed_writes"] += len(points)
    
    print(f"ğŸ’¾ å·²ä¿å­˜ {saved_count} ä¸ªæ•°æ®ç‚¹åˆ°æœ¬åœ°ç¼“å­˜")


# ============================================================
# ç¼“å­˜é‡è¯•ä»»åŠ¡
# ============================================================
async def _retry_cached_data():
    """å®šæœŸé‡è¯•æœ¬åœ°ç¼“å­˜çš„æ•°æ®"""
    global _stats
    
    cache = get_local_cache()
    retry_interval = 60  # æ¯ 60 ç§’é‡è¯•ä¸€æ¬¡
    
    while _is_running:
        await asyncio.sleep(retry_interval)
        
        # æ£€æŸ¥ InfluxDB å¥åº·çŠ¶æ€
        healthy, _ = check_influx_health()
        if not healthy:
            continue
        
        # è·å–å¾…é‡è¯•æ•°æ®
        pending = cache.get_pending_points(limit=100, max_retry=5)
        
        if not pending:
            continue
        
        print(f"ğŸ”„ å¼€å§‹é‡è¯• {len(pending)} æ¡ç¼“å­˜æ•°æ®...")
        
        # é‡æ–°æ„å»º Point å¯¹è±¡
        points = []
        ids = []
        
        for point_id, cached_point in pending:
            try:
                point = build_point(
                    cached_point.measurement,
                    cached_point.tags,
                    cached_point.fields,
                    datetime.fromisoformat(cached_point.timestamp)
                )
                if point:
                    points.append(point)
                    ids.append(point_id)
            except Exception as e:
                print(f"âš ï¸ é‡å»º Point å¤±è´¥: {e}")
        
        if not points:
            continue
        
        # æ‰¹é‡å†™å…¥
        success, err = write_points_batch(points)
        
        if success:
            cache.mark_success(ids)
            _stats["retry_success"] += len(points)
            _stats["last_retry_time"] = beijing_isoformat()
            print(f"âœ… é‡è¯•æˆåŠŸ: {len(points)} æ¡æ•°æ®å·²å†™å…¥ InfluxDB")
        else:
            cache.mark_retry(ids)
            print(f"âŒ é‡è¯•å¤±è´¥: {err}")


# ============================================================
# ğŸ”§ å®šæœŸæ¸…ç†ä»»åŠ¡ï¼ˆæ¯å°æ—¶æ‰§è¡Œï¼‰
# ============================================================
async def _periodic_cleanup():
    """å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜å’Œæ‰§è¡Œå†…å­˜ç»´æŠ¤"""
    cleanup_interval = 3600  # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
    
    while _is_running:
        await asyncio.sleep(cleanup_interval)
        
        try:
            # æ¸…ç†æœ¬åœ°ç¼“å­˜ä¸­è¶…è¿‡7å¤©çš„å¤±è´¥è®°å½•
            cache = get_local_cache()
            cache.cleanup_old(days=7)
            
            # è®°å½•å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰
            import gc
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            print(f"ğŸ§¹ å®šæœŸæ¸…ç†å®Œæˆ | è®¾å¤‡ç¼“å­˜: {len(_latest_data)} | é‡é‡å†å²(Rows): {len(_weight_queues)}")
        except Exception as e:
            print(f"âš ï¸ å®šæœŸæ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")


# ============================================================
# ä¸»è½®è¯¢å¾ªç¯
# ============================================================
async def _poll_data():
    """è½®è¯¢DBå—æ•°æ®å¹¶å†™å…¥InfluxDBï¼ˆåŠ¨æ€é…ç½®ï¼‰
    
    æ”¯æŒä¸¤ç§æ¨¡å¼:
    - æ­£å¸¸æ¨¡å¼: ä»çœŸå®PLCè¯»å–æ•°æ®
    - Mockæ¨¡å¼: ä½¿ç”¨MockDataGeneratorç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    """
    global _buffer_count, _stats, _latest_data, _latest_timestamp, _mock_generator
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½DBå—é…ç½®
    db_configs = _load_db_mappings()
    
    poll_count = 0
    
    # æ ¹æ®æ¨¡å¼åˆå§‹åŒ–æ•°æ®æº
    if settings.mock_mode:
        # Mockæ¨¡å¼ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨
        import sys
        sys.path.insert(0, str(Path(__file__).parent.parent.parent / "tests" / "mock"))
        from mock_data_generator import MockDataGenerator
        _mock_generator = MockDataGenerator()
        print("ğŸ­ Mockæ¨¡å¼å·²å¯ç”¨ - ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        plc = None
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨çœŸå®PLC
        plc = get_plc_manager()
    
    while _is_running:
        poll_count += 1
        timestamp = now_beijing()
        _stats["total_polls"] += 1
        
        try:
            # ============================================================
            # Step 1: Initialize Mock Data (if needed)
            # ============================================================
            if settings.mock_mode and _mock_generator:
                # Mockæ¨¡å¼ï¼šä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰DBå—æ•°æ®
                mock_db_data = _mock_generator.generate_all_db_data()
                
                # ä¿å­˜mock_db_dataä¾›Step 2ä½¿ç”¨ï¼ˆé¿å…é‡å¤ç”Ÿæˆï¼‰
                _current_mock_db_data = mock_db_data
            else:
                _current_mock_db_data = None
            
            # ============================================================
            # Step 2: è¯»å–æ‰€æœ‰ DB å—æ•°æ®
            # ============================================================
            all_devices = []
            
            if settings.mock_mode and _mock_generator:
                # Mockæ¨¡å¼ï¼šä½¿ç”¨Step 1å·²ç”Ÿæˆçš„æ•°æ®ï¼ˆé¿å…é‡å¤è°ƒç”¨generate_all_db_dataï¼‰
                mock_db_data = _current_mock_db_data if _current_mock_db_data else _mock_generator.generate_all_db_data()
                
                for db_num, size in db_configs:
                    db_data = mock_db_data.get(db_num)
                    if db_data is None:
                        continue
                    
                    # è§£æè®¾å¤‡æ•°æ®
                    if db_num in _parsers:
                        devices = _parsers[db_num].parse_all(db_data)
                        all_devices.extend(devices)
                        
                        # æ›´æ–°å†…å­˜ç¼“å­˜
                        for device in devices:
                            _update_latest_data(device, db_num, timestamp)
            else:
                # æ­£å¸¸æ¨¡å¼ï¼šä»PLCè¯»å–æ•°æ®
                for db_num, size in db_configs:
                    # ä½¿ç”¨é•¿è¿æ¥è¯»å–
                    success, db_data, err = plc.read_db(db_num, 0, size)
                    
                    if not success:
                        print(f"âŒ DB{db_num} è¯»å–å¤±è´¥: {err}")
                        continue
                    
                    # è§£æè®¾å¤‡æ•°æ®
                    if db_num in _parsers:
                        devices = _parsers[db_num].parse_all(db_data)
                        all_devices.extend(devices)
                        
                        # æ›´æ–°å†…å­˜ç¼“å­˜
                        for device in devices:
                            _update_latest_data(device, db_num, timestamp)
            
            # æ›´æ–°æ—¶é—´æˆ³
            _latest_timestamp = timestamp
            
            # å°†æ•°æ®åŠ å…¥å†™å…¥ç¼“å†²åŒº
            written_count = 0
            for device in all_devices:
                count = _add_device_to_buffer(device, all_devices[0].get('db_number', 8) if all_devices else 8, timestamp)
                written_count += count
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡å†™å…¥
            _buffer_count += 1
            
            # ğŸ”§ [CRITICAL] ç¼“å†²åŒºå‘Šè­¦é˜ˆå€¼é™ä½ï¼ˆé˜²æ­¢é˜»å¡ API è¯·æ±‚ï¼‰
            buffer_usage = len(_point_buffer) / 1000
            if buffer_usage > 0.5:  # 50% å‘Šè­¦ï¼ˆä» 80% é™ä½ï¼‰
                print(f"âš ï¸ ç¼“å†²åŒºä½¿ç”¨ç‡è¿‡é«˜: {buffer_usage*100:.1f}% (å°†è§¦å‘æ‰¹é‡å†™å…¥)")
            
            # ğŸ”§ [CRITICAL] è§¦å‘æ‰¹é‡å†™å…¥ï¼šè¾¾åˆ°æ‰¹æ¬¡æ•°æˆ–ç¼“å†²åŒº>500ä¸ªç‚¹ï¼ˆé˜²æ­¢é˜»å¡è¿‡ä¹…ï¼‰
            # åŸ: _buffer_count >= 20 or len(_point_buffer) >= 800
            # æ–°: _buffer_count >= 12 or len(_point_buffer) >= 500
            if _buffer_count >= _batch_size or len(_point_buffer) >= 500:
                _flush_buffer()
            
            # æ—¥å¿—è¾“å‡º
            if settings.verbose_polling_log or poll_count % 10 == 0:
                cache_stats = get_local_cache().get_stats()
                print(f"ğŸ“Š [poll #{poll_count}] "
                      f"è®¾å¤‡: {len(all_devices)} | "
                      f"æ•°æ®ç‚¹: {written_count} | "
                      f"ç¼“å†²åŒº={len(_point_buffer)}/{_batch_size} | "
                      f"å¾…é‡è¯•={cache_stats['pending_count']}")
        
        except Exception as e:
            print(f"âŒ [poll #{poll_count}] è½®è¯¢å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
        
        # ä½¿ç”¨è¿è¡Œæ—¶é…ç½®çš„è½®è¯¢é—´éš”ï¼ˆæ”¯æŒçƒ­æ›´æ–°ï¼‰
        try:
            from app.routers.config import get_runtime_plc_config
            plc_config = get_runtime_plc_config()
            await asyncio.sleep(plc_config["poll_interval"])
        except:
            await asyncio.sleep(_poll_interval)


# ============================================================
# æ›´æ–°å†…å­˜ç¼“å­˜ï¼ˆä¾› API ç›´æ¥è¯»å–ï¼‰
# ============================================================
def _update_latest_data(device_data: Dict[str, Any], db_number: int, timestamp: datetime):
    """æ›´æ–°å†…å­˜ç¼“å­˜ä¸­çš„æœ€æ–°æ•°æ®
    
    Args:
        device_data: è§£æåçš„è®¾å¤‡æ•°æ®
        db_number: DBå—å·
        timestamp: æ—¶é—´æˆ³
    """
    global _latest_data
    
    device_id = device_data['device_id']
    device_type = device_data['device_type']
    
    # è½¬æ¢æ‰€æœ‰æ¨¡å—æ•°æ®
    modules_data = {}
    
    for module_tag, module_data in device_data['modules'].items():
        module_type = module_data['module_type']
        raw_fields = module_data['fields']
        
        # ä½¿ç”¨è½¬æ¢å™¨è½¬æ¢æ•°æ®
        if module_type in CONVERTER_MAP:
            converter = get_converter(module_type)
            fields = converter.convert(raw_fields)
        else:
            # æœªçŸ¥æ¨¡å—ç±»å‹ï¼Œç›´æ¥æå–åŸå§‹å€¼
            fields = {}
            for field_name, field_info in raw_fields.items():
                fields[field_name] = field_info['value']
        
        modules_data[module_tag] = {
            "module_type": module_type,
            "fields": fields
        }
    
    # æ›´æ–°å†…å­˜ç¼“å­˜
    with _data_lock:  # ğŸ”§ çº¿ç¨‹å®‰å…¨å†™å…¥
        _latest_data[device_id] = {
            "device_id": device_id,
            "device_type": device_type,
            "db_number": str(db_number),
            "timestamp": timestamp.isoformat(),
            "modules": modules_data
        }


def _update_status_cache(status_data: bytes, status_parser):
    """æ›´æ–°çŠ¶æ€ä½å†…å­˜ç¼“å­˜
    
    Args:
        status_data: çŠ¶æ€DBå—çš„åŸå§‹å­—èŠ‚æ•°æ®
        status_parser: çŠ¶æ€è§£æå™¨å®ä¾‹
    """
    global _latest_status
    
    # è·å–æ‰€æœ‰è®¾å¤‡çš„çŠ¶æ€
    success_list, error_list = status_parser.check_all_status(status_data)
    
    # åˆå¹¶æˆåŠŸå’Œé”™è¯¯åˆ—è¡¨ï¼Œæ›´æ–°ç¼“å­˜
    all_status = success_list + error_list
    
    for status in all_status:
        device_id = status['device_id']
        _latest_status[device_id] = {
            "device_id": device_id,
            "device_type": status['device_type'],
            "description": status.get('description', ''),
            "done": status['done'],
            "busy": status['busy'],
            "error": status['error'],
            "status_code": status['status_code'],
            "timestamp": now_beijing().isoformat()
        }


# ============================================================
# å°†è®¾å¤‡æ•°æ®åŠ å…¥å†™å…¥ç¼“å†²åŒº
# ============================================================
def _add_device_to_buffer(device_data: Dict[str, Any], db_number: int, timestamp: datetime) -> int:
    """å°†è®¾å¤‡æ•°æ®åŠ å…¥å†™å…¥ç¼“å†²åŒº
    
    Args:
        device_data: è§£æåçš„è®¾å¤‡æ•°æ®
        db_number: DBå—å·
        timestamp: æ—¶é—´æˆ³
    
    Returns:
        æ·»åŠ çš„æ•°æ®ç‚¹æ•°é‡
    """
    device_id = device_data['device_id']
    device_type = device_data['device_type']
    point_count = 0
    
    # éå†æ‰€æœ‰æ¨¡å—
    for module_tag, module_data in device_data['modules'].items():
        module_type = module_data['module_type']
        raw_fields = module_data['fields']
        
        # ä½¿ç”¨è½¬æ¢å™¨è½¬æ¢æ•°æ®
        if module_type in CONVERTER_MAP:
            converter = get_converter(module_type)
            fields = converter.convert(raw_fields)
        else:
            # æœªçŸ¥æ¨¡å—ç±»å‹ï¼Œç›´æ¥æå–åŸå§‹å€¼
            fields = {}
            for field_name, field_info in raw_fields.items():
                fields[field_name] = field_info['value']
        
        # è·³è¿‡ç©ºå­—æ®µ
        if not fields:
            continue
        
        # æ„å»º Point å¯¹è±¡
        point = build_point(
            measurement="sensor_data",
            tags={
                "device_id": device_id,
                "device_type": device_type,
                "module_type": module_type,
                "module_tag": module_tag,
                "db_number": str(db_number)
            },
            fields=fields,
            timestamp=timestamp
        )
        
        if point:
            _point_buffer.append(point)
            point_count += 1
    
    return point_count


# ------------------------------------------------------------
# 3. start_polling() - å¯åŠ¨æ•°æ®è½®è¯¢ä»»åŠ¡
# ------------------------------------------------------------
async def start_polling():
    """å¯åŠ¨æ•°æ®è½®è¯¢ä»»åŠ¡ï¼ˆä»é…ç½®æ–‡ä»¶åŠ¨æ€åŠ è½½ï¼‰"""
    global _polling_task, _retry_task, _is_running, _batch_size, _poll_interval, _write_queue, _write_task
    
    if _is_running:
        print("âš ï¸ è½®è¯¢æœåŠ¡å·²åœ¨è¿è¡Œ")
        return
    
    # åŠ è½½DBæ˜ å°„é…ç½®
    _load_db_mappings()
    
    # åŠ¨æ€åˆå§‹åŒ–è§£æå™¨
    print("ğŸ“¦ åˆå§‹åŒ–è§£æå™¨:")
    _init_parsers()
    
    _is_running = True
    
    # ğŸ”§ [NEW] åˆå§‹åŒ–å¼‚æ­¥å†™å…¥é˜Ÿåˆ—ï¼ˆæœ€å¤šç¼“å­˜ 10 æ‰¹æ•°æ®ï¼‰
    _write_queue = asyncio.Queue(maxsize=10)
    
    # æ ¹æ®æ¨¡å¼å¯åŠ¨
    if settings.mock_mode:
        print("ğŸ­ Mockæ¨¡å¼ - è·³è¿‡PLCè¿æ¥")
    else:
        # å¯åŠ¨ PLC é•¿è¿æ¥
        plc = get_plc_manager()
        success, err = plc.connect()
        if success:
            print(f"âœ… PLC é•¿è¿æ¥å·²å»ºç«‹")
        else:
            print(f"âš ï¸ PLC è¿æ¥å¤±è´¥: {err}ï¼Œå°†åœ¨è½®è¯¢æ—¶é‡è¯•")
    
    # ğŸ”§ [NEW] å¯åŠ¨åå°å†™å…¥ä»»åŠ¡ï¼ˆå…³é”®ï¼šä¸é˜»å¡ APIï¼‰
    _write_task = asyncio.create_task(_background_writer())
    
    # å¯åŠ¨è½®è¯¢ä»»åŠ¡
    _polling_task = asyncio.create_task(_poll_data())
    _retry_task = asyncio.create_task(_retry_cached_data())
    _cleanup_task = asyncio.create_task(_periodic_cleanup())  # ğŸ”§ å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
    
    mode_str = "Mockæ¨¡å¼" if settings.mock_mode else "æ­£å¸¸æ¨¡å¼"
    print(f"âœ… è½®è¯¢æœåŠ¡å·²å¯åŠ¨ ({mode_str}, é—´éš”: {_poll_interval}s, æ‰¹é‡: {_batch_size}æ¬¡)")
    print(f"ğŸš€ åå°å†™å…¥æ¨¡å¼å·²å¯ç”¨ - API è¯·æ±‚ä¸ä¼šè¢«é˜»å¡")


# ------------------------------------------------------------
# 4. stop_polling() - åœæ­¢æ•°æ®è½®è¯¢ä»»åŠ¡
# ------------------------------------------------------------
async def stop_polling():
    """åœæ­¢æ•°æ®è½®è¯¢ä»»åŠ¡"""
    global _polling_task, _retry_task, _cleanup_task, _write_task, _is_running, _write_queue
    
    _is_running = False
    
    # åˆ·æ–°ç¼“å†²åŒºï¼ˆå°†å‰©ä½™æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼‰
    print("â³ æ­£åœ¨åˆ·æ–°ç¼“å†²åŒº...")
    _flush_buffer()
    
    # ğŸ”§ [NEW] ç­‰å¾…å†™å…¥é˜Ÿåˆ—å¤„ç†å®Œæˆï¼ˆæœ€å¤šç­‰å¾… 10 ç§’ï¼‰
    if _write_queue is not None:
        try:
            await asyncio.wait_for(_write_queue.join(), timeout=10.0)
            print("âœ… å†™å…¥é˜Ÿåˆ—å·²æ¸…ç©º")
        except asyncio.TimeoutError:
            print("âš ï¸ å†™å…¥é˜Ÿåˆ—æ¸…ç©ºè¶…æ—¶ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½ä¸¢å¤±")
    
    # ğŸ”§ å–æ¶ˆæ‰€æœ‰ä»»åŠ¡ï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
    tasks_to_cancel = [
        ("polling", _polling_task), 
        ("retry", _retry_task), 
        ("cleanup", _cleanup_task),
        ("writer", _write_task)  # ğŸ”§ [NEW] åå°å†™å…¥ä»»åŠ¡
    ]
    
    for task_name, task in tasks_to_cancel:
        if task:
            task.cancel()
            try:
                await asyncio.wait_for(task, timeout=5.0)  # ğŸ”§ æœ€å¤šç­‰å¾…5ç§’
            except asyncio.CancelledError:
                pass
            except asyncio.TimeoutError:
                print(f"âš ï¸ {task_name} ä»»åŠ¡å–æ¶ˆè¶…æ—¶ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
    
    _polling_task = None
    _retry_task = None
    _cleanup_task = None
    _write_task = None  # ğŸ”§ [NEW] é‡ç½®å†™å…¥ä»»åŠ¡å¥æŸ„
    _write_queue = None  # ğŸ”§ [NEW] é‡ç½®å†™å…¥é˜Ÿåˆ—
    
    # æ–­å¼€ PLC é•¿è¿æ¥
    plc = get_plc_manager()
    plc.disconnect()
    
    print("â¹ï¸ è½®è¯¢æœåŠ¡å·²åœæ­¢")


# ============================================================
# API æŸ¥è¯¢å‡½æ•°ï¼ˆä¾› Router ä½¿ç”¨ï¼‰
# ============================================================
def get_latest_data() -> Dict[str, Any]:
    """è·å–æ‰€æœ‰è®¾å¤‡çš„æœ€æ–°æ•°æ®ï¼ˆä»å†…å­˜ç¼“å­˜ï¼‰
    
    Returns:
        {device_id: {device_id, device_type, timestamp, modules: {...}}}
    """
    with _data_lock:  # ğŸ”§ çº¿ç¨‹å®‰å…¨è¯»å–
        return _latest_data.copy()


def get_latest_device_data(device_id: str) -> Optional[Dict[str, Any]]:
    """è·å–å•ä¸ªè®¾å¤‡çš„æœ€æ–°æ•°æ®ï¼ˆä»å†…å­˜ç¼“å­˜ï¼‰
    
    Args:
        device_id: è®¾å¤‡ID
    
    Returns:
        è®¾å¤‡æ•°æ®æˆ– None
    """
    with _data_lock:  # ğŸ”§ çº¿ç¨‹å®‰å…¨è¯»å–
        return _latest_data.get(device_id)


def get_latest_devices_by_type(device_type: str) -> List[Dict[str, Any]]:
    """è·å–æŒ‡å®šç±»å‹çš„æ‰€æœ‰è®¾å¤‡æœ€æ–°æ•°æ®
    
    Args:
        device_type: è®¾å¤‡ç±»å‹ (short_hopper, long_hopper, etc.)
    
    Returns:
        è®¾å¤‡æ•°æ®åˆ—è¡¨
    """
    with _data_lock:  # ğŸ”§ çº¿ç¨‹å®‰å…¨è¯»å–
        return [
            data for data in _latest_data.values()
            if data.get('device_type') == device_type
        ]


def get_latest_timestamp() -> Optional[str]:
    """è·å–æœ€æ–°æ•°æ®çš„æ—¶é—´æˆ³"""
    return _latest_timestamp.isoformat() if _latest_timestamp else None


def is_polling_running() -> bool:
    """æ£€æŸ¥è½®è¯¢æœåŠ¡æ˜¯å¦åœ¨è¿è¡Œ"""
    return _is_running


def get_polling_stats() -> Dict[str, Any]:
    """è·å–è½®è¯¢ç»Ÿè®¡ä¿¡æ¯"""
    cache_stats = get_local_cache().get_stats()
    plc_status = get_plc_manager().get_status()
    
    return {
        **_stats,
        "buffer_size": len(_point_buffer),
        "batch_size": _batch_size,
        "devices_in_cache": len(_latest_data),
        "latest_timestamp": get_latest_timestamp(),
        "cache_stats": cache_stats,
        "plc_status": plc_status
    }
