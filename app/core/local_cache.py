# ============================================================
# æ–‡ä»¶è¯´æ˜: local_cache.py - æœ¬åœ° SQLite é™çº§ç¼“å­˜
# ============================================================
# 1, å½“ InfluxDB ä¸å¯ç”¨æ—¶ï¼Œæ•°æ®æš‚å­˜åˆ°æœ¬åœ° SQLite
# 2, InfluxDB æ¢å¤åè‡ªåŠ¨é‡è¯•å†™å…¥
# ============================================================

import sqlite3
import json
import threading
import atexit
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict

from config import get_settings

settings = get_settings()

# 3, ç¼“å­˜æ–‡ä»¶è·¯å¾„
CACHE_DB_PATH = Path(getattr(settings, 'local_cache_path', 'data/cache.db'))

# 4, æ¨¡å—çº§å•ä¾‹ (é¿å…ç±»çº§åˆ«ç«æ€æ¡ä»¶)
_cache_instance: Optional['LocalCache'] = None
_cache_lock = threading.Lock()


@dataclass
class CachedPoint:
    """ç¼“å­˜çš„æ•°æ®ç‚¹"""
    measurement: str
    tags: Dict[str, str]
    fields: Dict[str, Any]
    timestamp: str  # ISO æ ¼å¼
    retry_count: int = 0
    created_at: str = ""
    
    def to_json(self) -> str:
        return json.dumps(asdict(self), ensure_ascii=False)
    
    @classmethod
    def from_json(cls, json_str: str) -> 'CachedPoint':
        data = json.loads(json_str)
        return cls(**data)


class LocalCache:
    """æœ¬åœ° SQLite ç¼“å­˜ç®¡ç†å™¨
    
    # 5, ä½¿ç”¨ WAL æ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½
    # 6, çº¿ç¨‹é”ä¿æŠ¤æ•°æ®åº“æ“ä½œ
    """
    
    def __init__(self):
        self._conn: Optional[sqlite3.Connection] = None
        # 6, çº¿ç¨‹é”ä¿æŠ¤æ•°æ®åº“æ“ä½œ
        self._db_lock = threading.Lock()
        self._init_db()
        
        # 7, æ³¨å†Œé€€å‡ºæ—¶å…³é—­è¿æ¥
        atexit.register(self.close)
    
    def _init_db(self) -> None:
        """åˆå§‹åŒ– SQLite æ•°æ®åº“
        
        # 5, WAL æ¨¡å¼é…ç½®æé«˜å¹¶å‘æ€§èƒ½
        """
        CACHE_DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        
        # 5, WAL æ¨¡å¼é…ç½®
        self._conn = sqlite3.connect(str(CACHE_DB_PATH), check_same_thread=False, timeout=30)
        self._conn.execute("PRAGMA journal_mode=WAL")
        self._conn.execute("PRAGMA synchronous=NORMAL")
        self._conn.execute("PRAGMA cache_size=10000")
        
        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS pending_points (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                measurement TEXT NOT NULL,
                data_json TEXT NOT NULL,
                retry_count INTEGER DEFAULT 0,
                created_at TEXT NOT NULL,
                last_retry_at TEXT
            )
        """)
        self._conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_measurement ON pending_points(measurement)
        """)
        self._conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_created_at ON pending_points(created_at)
        """)
        self._conn.commit()
        
        # ç»Ÿè®¡å¾…å¤„ç†æ•°æ®
        cursor = self._conn.execute("SELECT COUNT(*) FROM pending_points")
        count = cursor.fetchone()[0]
        if count > 0:
            print(f"âš ï¸ æœ¬åœ°ç¼“å­˜æœ‰ {count} æ¡å¾…å†™å…¥æ•°æ®")
    
    def save_points(self, points: List[CachedPoint]) -> int:
        """ä¿å­˜æ•°æ®ç‚¹åˆ°æœ¬åœ°ç¼“å­˜
        
        # 1, å½“ InfluxDB ä¸å¯ç”¨æ—¶çš„é™çº§å­˜å‚¨
        # 6, ä½¿ç”¨é”ä¿è¯çº¿ç¨‹å®‰å…¨
        
        Args:
            points: æ•°æ®ç‚¹åˆ—è¡¨
        
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        if not points:
            return 0
        
        # 6, ä½¿ç”¨é”ä¿è¯çº¿ç¨‹å®‰å…¨
        with self._db_lock:
            try:
                now = datetime.now(timezone.utc).isoformat()
                data = [(p.measurement, p.to_json(), p.retry_count, now) for p in points]
                self._conn.executemany(
                    "INSERT INTO pending_points (measurement, data_json, retry_count, created_at) VALUES (?, ?, ?, ?)",
                    data
                )
                self._conn.commit()
                return len(points)
            except Exception as e:
                print(f"âŒ æœ¬åœ°ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
                return 0
    
    def get_pending_points(self, limit: int = 100, max_retry: int = 5) -> List[tuple]:
        """è·å–å¾…é‡è¯•çš„æ•°æ®ç‚¹
        
        # 2, InfluxDB æ¢å¤åè‡ªåŠ¨é‡è¯•å†™å…¥
        
        Args:
            limit: æœ€å¤§è·å–æ•°é‡
            max_retry: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            [(id, CachedPoint), ...]
        """
        with self._db_lock:
            try:
                cursor = self._conn.execute(
                    "SELECT id, data_json FROM pending_points WHERE retry_count < ? ORDER BY created_at ASC LIMIT ?",
                    (max_retry, limit)
                )
                results = []
                for row in cursor.fetchall():
                    try:
                        point = CachedPoint.from_json(row[1])
                        results.append((row[0], point))
                    except Exception:
                        pass  # è·³è¿‡è§£æå¤±è´¥çš„è®°å½•
                return results
            except Exception as e:
                print(f"âŒ è¯»å–æœ¬åœ°ç¼“å­˜å¤±è´¥: {e}")
                return []
    
    def mark_success(self, ids: List[int]) -> None:
        """æ ‡è®°æ•°æ®ç‚¹å†™å…¥æˆåŠŸï¼ˆåˆ é™¤ï¼‰"""
        if not ids:
            return
        with self._db_lock:
            try:
                placeholders = ",".join("?" * len(ids))
                self._conn.execute(f"DELETE FROM pending_points WHERE id IN ({placeholders})", ids)
                self._conn.commit()
            except Exception as e:
                print(f"âŒ åˆ é™¤ç¼“å­˜è®°å½•å¤±è´¥: {e}")
    
    def mark_retry(self, ids: List[int]) -> None:
        """æ ‡è®°æ•°æ®ç‚¹éœ€è¦é‡è¯•ï¼ˆå¢åŠ é‡è¯•è®¡æ•°ï¼‰"""
        if not ids:
            return
        with self._db_lock:
            try:
                now = datetime.now(timezone.utc).isoformat()
                placeholders = ",".join("?" * len(ids))
                self._conn.execute(
                    f"UPDATE pending_points SET retry_count = retry_count + 1, last_retry_at = ? WHERE id IN ({placeholders})",
                    [now] + ids
                )
                self._conn.commit()
            except Exception as e:
                print(f"âŒ æ›´æ–°é‡è¯•è®¡æ•°å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._db_lock:
            try:
                cursor = self._conn.execute("""
                    SELECT 
                        COUNT(*) as total,
                        SUM(CASE WHEN retry_count >= 5 THEN 1 ELSE 0 END) as failed,
                        MIN(created_at) as oldest
                    FROM pending_points
                """)
                row = cursor.fetchone()
                return {
                    "pending_count": row[0] or 0,
                    "failed_count": row[1] or 0,
                    "oldest_record": row[2]
                }
            except Exception:
                return {"pending_count": 0, "failed_count": 0, "oldest_record": None}
    
    def cleanup_old(self, days: int = 7) -> None:
        """æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„å¤±è´¥è®°å½•"""
        with self._db_lock:
            try:
                cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
                cursor = self._conn.execute(
                    "DELETE FROM pending_points WHERE created_at < ? AND retry_count >= 5",
                    (cutoff,)
                )
                deleted = cursor.rowcount
                self._conn.commit()
                if deleted > 0:
                    print(f"ğŸ§¹ æ¸…ç†äº† {deleted} æ¡è¿‡æœŸç¼“å­˜è®°å½•")
            except Exception as e:
                print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def close(self) -> None:
        """å…³é—­æ•°æ®åº“è¿æ¥
        
        # 7, é€€å‡ºæ—¶è‡ªåŠ¨è°ƒç”¨ (atexit æ³¨å†Œ)
        """
        if self._conn:
            try:
                self._conn.close()
            except Exception:
                pass
            self._conn = None


def get_local_cache() -> LocalCache:
    """è·å–æœ¬åœ°ç¼“å­˜å•ä¾‹
    
    # 4, ä½¿ç”¨æ¨¡å—çº§é”é¿å…ç«æ€æ¡ä»¶
    """
    global _cache_instance
    if _cache_instance is None:
        with _cache_lock:
            if _cache_instance is None:
                _cache_instance = LocalCache()
    return _cache_instance
